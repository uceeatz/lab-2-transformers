{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rezQ0tp6T-Hm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0TlkLPJJBdH",
        "outputId": "406fbca9-c99c-4b42-df0d-6c3dc10d5c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.8/dist-packages (0.4.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.3.5)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.9.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (21.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.12.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2022.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->evaluate) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2022.7)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcSJITklJE5E"
      },
      "source": [
        "#Section 1 - Michael"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5cnpmA4MCtX"
      },
      "source": [
        "# A Brief History of Transformers\n",
        "\n",
        "(Expected reading time: 10 mins)\n",
        "\n",
        "Before getting started on your tasks, this section provides an overview of:\n",
        "* The Transformer architecture\n",
        "* How it achieves state-of-the-art performance in a range of tasks\n",
        "* How it has developed in the years since its introduction\n",
        "\n",
        "---\n",
        "\n",
        "### What makes Transformers special?\n",
        "\n",
        "Transformer originally designed as an architecture for language models but have since been adapated for a wide variety of tasks e.g. machine vision.\n",
        "\n",
        "In the previous lab, you learnt that RNNs are useful for NLP but they have two major shortcomings:\n",
        "\n",
        "1. RNNs don't parallelize well\n",
        "2. RNNs struggle to capture long-range dependencies\n",
        "\n",
        "Since their introduction in 2018, Transformer architectures have been used to achieve state-of-the-art performance in NLP tasks. This is because they are able to overcome the difficulties faced by RNNs. \n",
        "\n",
        "| RNN Problem             | Transformer Solution |\n",
        "|-------------------------|----------------------|\n",
        "| Parallelization         | Positional encoding  |\n",
        "| Long-range dependencies | Sef-attention mechanism  |\n",
        "\n",
        "Explanation of positional encoding:\n",
        "\n",
        "Explanation of attention:\n",
        "Explanation of attention + multi-head attention.\n",
        "So central is attention to the success of Transformers that the original 2018 paper that introduced the architecture was called [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "An in-depth understanding of the Transformer architecture is not necessary to complete the tasks in this lab.\n",
        "\n",
        "---\n",
        "\n",
        "### What does a Transformer look like?\n",
        "\n",
        "The original diagram of the Transformer architecture shows how it incorporates the ideas of positional encoding and attention.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1bD8TnqfqnpnMTDwnXTDdtJ2EKvC0rEQa)\n",
        "\n",
        "The encoder and decoder blocks serve the following purposes:\n",
        "\n",
        "- Encoder\n",
        "- Decoder\n",
        "\n",
        "A Transformer can consist of one or both blocks, depending on the required task.\n",
        "\n",
        "\n",
        "| Transformer Type | Use Case                                                                                                              |\n",
        "|------------------|-----------------------------------------------------------------------------------------------------------------------|\n",
        "| Encoder-only     | Good for tasks that require understanding of the input,  such as sentence classification and named entity recognition |\n",
        "| Decoder-only     | Good for generative tasks such as text generation                                                                     |\n",
        "| Encoder-Decoder  | Good for generative tasks that require an input,  such as translation or summarization                                |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNMp8B-DMHxo"
      },
      "source": [
        "# How can we use Transformers?\n",
        "\n",
        "Explain how the models are huge, required massive amount of compute and therefore time and energy.\n",
        "\n",
        "But we can use pre-trained models!\n",
        "\n",
        "Explanation of transfer learning and fine-tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KECBO7K9a6IM"
      },
      "source": [
        "# Huggingface 🤗\n",
        "\n",
        "#### Need to include some installation instructions here perhaps?\n",
        "\n",
        "### Pipelines\n",
        "\n",
        "### Models\n",
        "\n",
        "### Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2GW8cGfJGci"
      },
      "source": [
        "#Section 2 - Will"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbtcpjS5g5c3"
      },
      "source": [
        "# Training Transformer Models\n",
        "\n",
        "In the previous section students will have:\n",
        "- Worked with transformer pipelines \n",
        "- Loaded datasets, tokenizers and models\n",
        "\n",
        "In this section the student will:\n",
        "- Understand the difference between fine tuning and pre-training\n",
        "- Fine tune a pre-trained transformer model from scratch as per the example code from the hugging face tutorial on a text classification setting.\n",
        "- Compare the fine tuned model results to those directly from a pipeline \n",
        "\n",
        "If time:\n",
        "- Learn how to create a custom dataset loader, such that they can easily adapt any dataset for their project work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re2o-fsXdaXB"
      },
      "source": [
        "# Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyO_VtXndeZv"
      },
      "source": [
        "For this task we will give you three seperate datasets to investigate. Firstly look at each of these datasets and understand what they consist of and what NLP tasks you could finetune using these datasets.\n",
        "\n",
        "Therefore, we ask you to do the following:\n",
        "\n",
        "\n",
        "**1. Look at the three datasets below on Hugging Face and investigate them thouroughly. Understand these following aspects before you move on:**\n",
        "* What features do the datasets contain?\n",
        "* Are the datasets already tokenised or do they contain text?\n",
        "* What are suitable tasks to train these datasets on? (e.g. Token classification, sentiment analysis, sequence classification, masked language modelling)\n",
        "\n",
        "**2. Choose one of these datasets to finetune a model.**\n",
        "> Understand what the task it is you are going to fine tune on given the dataset.\n",
        "\n",
        "**3. Choose a model to finetune on this dataset.**\n",
        "\n",
        "> Use the hugging face documentation to choose a correct model ([HuggingFace models](https://huggingface.co/models))\n",
        "\n",
        "**4.   Pre-process the dataset to train the model.**\n",
        "> Understanding exactly what task you are going to be finetuning the model for, will help a lot here. Think about what is it the model needs as an input and see how you need to change the given features to these inputs. Use Hugging Face tokenisers, data collators and general documentation to figure this out.\n",
        "\n",
        "**5. Train the model on this dataset.**\n",
        "> Use a manual training loop here, understand the mechanics behind training and implement it yourself (you can find this in the Hugging Face documentation).\n",
        "\n",
        "**6. Evaluate the new model's performance - compare with the performance of the model before finetuning.**\n",
        "> Look at what metric you would use to measure the performance of the model, this can be tricky for some language modelling tasks with non-deterministic labels.\n",
        "\n",
        "\n",
        "The datasets:\n",
        "\n",
        "1. [tweet_eval](https://huggingface.co/datasets/tweet_eval)\n",
        "2. [wikitext](https://huggingface.co/datasets/wikitext)\n",
        "3. [wikiann](https://huggingface.co/datasets/wikiann)\n",
        "\n",
        "Models available at: https://huggingface.co/models"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}