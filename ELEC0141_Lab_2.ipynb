{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uceeatz/lab-2-transformers/blob/main/ELEC0141_Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0TlkLPJJBdH"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcSJITklJE5E"
      },
      "source": [
        "---\n",
        "#**Section 1 - Introduction**\n",
        "___\n",
        "\n",
        "*Transformer, a powerful tool,*\n",
        "\n",
        "*With Hugging Face, an online jewel.*\n",
        "\n",
        "*Pre-trained models, a great base,*\n",
        "\n",
        "*Fine-tune them, you'll win the race.*\n",
        "\n",
        "*Self-attention, multi-head,*\n",
        "\n",
        "*It's the future, stay ahead.*\n",
        "\n",
        "*Encoder, decoder blocks,*\n",
        "\n",
        "*In NLP, it unlocks,*\n",
        "\n",
        "*The state-of-the-art, it's true,*\n",
        "\n",
        "*With Transformer, you'll shine through.*\n",
        "\n",
        "\\- **A short poem about Transformers, written by a Transformer (*ChatGPT*)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.1 - A Brief History of Transformers**\n",
        "\n",
        "*(Expected reading time: 10 mins)*\n",
        "\n",
        "Before getting started on your tasks, this section provides an overview of:\n",
        "* The Transformer architecture\n",
        "* How Transformer models achieve state-of-the-art performance in a range of NLP tasks\n",
        "* Which variants of the Transformer archicture are used for different tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "fqHsE86SyYSh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5cnpmA4MCtX"
      },
      "source": [
        "---\n",
        "### **1.11 - What makes Transformers so successful?**\n",
        "\n",
        "In Lab 1, you learnt that RNNs are useful for NLP but they have two major shortcomings:\n",
        "\n",
        "1. **RNNs don't parallelize well** due to their dependence on sequential processing.\n",
        "\n",
        "2. **RNNs struggle to capture long-range dependencies** due to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) and the limited memory of the hidden state.\n",
        "\n",
        "Since their introduction in 2018, Transformer models have overcome these limitations of RNNs and achieved state-of-the-art performance in NLP tasks. The table below summarises the Transformer's key innovations.\n",
        "\n",
        "\n",
        "|  RNN Problem             | Transformer Solution |\n",
        "|-------------------------|----------------------|\n",
        "| <img width=100/> Parallelization <img width=100/> | <img width=100/> Positional encoding <img width=100/> |\n",
        "| <img width=100/> Long-range dependencies <img width=100/> | <img width=100/> Sef-attention mechanism <img width=100/> |\n",
        "\n",
        "\n",
        "Attention mechanisms are so central to the success of Transformers that the original 2018 paper that introduced the architecture was called [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "While an in-depth understanding of the Transformer architecture is not necessary to complete the tasks in this lab, it is useful to have an understanding of the core concepts that make Tranformer models the state-of-the-art in ML and NLP:\n",
        "\n",
        "\n",
        "**Positional encoding** is a technique used in the Transformer architecture to give the model an understanding of the position of the elements in the input sequence, while allowing them to be processed in parallel. It's implemented by adding a learned vector to the input embedding at each position of the input sequence, which encodes the position information of the element in the input sequence.\n",
        "\n",
        "**Attention** is a mechanism that allows a model to weigh the importance of different elements of the input when processing it. Attention mechanisms calculate a set of attention weights that indicate how much each element of the input should be considered when computing the final output.\n",
        "\n",
        "**Self-attention** is a specific type of attention mechanism used in the encoder of Transformer models to weigh the importance of different parts of the input sequence. For NLP, this allows words to be understood in the context of phrases, sentences, paragraphs, etc.\n",
        "\n",
        "**Multi-head self-attention** is an extension of the single-head self-attention mechanism, where the model computes multiple sets of attention weights, each using a different set of learned parameters. It helps the model capture more complex relationships between the input elements. Multi-head self-attention is used in the encoder of Transformer models.\n",
        "\n",
        "For further reading (and pictures) on these concepts, check out the excellent guide [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **1.12 - What does a Transformer look like?**\n",
        "\n",
        "The original diagram of the Transformer architecture shows how it incorporates the ideas of positional encoding and attention.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1bD8TnqfqnpnMTDwnXTDdtJ2EKvC0rEQa)\n",
        "\n",
        "\n",
        "The encoder and decoder blocks serve the following purposes:\n",
        "\n",
        "- **Encoder**: The encoder block is responsible for encoding the input sequence into a hidden representation that captures the key information in the input. The encoder block is typically composed of a stack of identical layers, each layer consisting of a multi-head self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the encoder to weigh the importance of different parts of the input sequence when encoding it, and the feed-forward neural network helps the encoder to extract more complex features from the input.\n",
        "\n",
        "- **Decoder**: The decoder block, on the other hand, is responsible for decoding the encoded input representation into an output sequence. Like the encoder, the decoder block is also typically composed of a stack of identical layers, but in addition to the multi-head self-attention mechanism and feed-forward neural network, it also includes a multi-head attention mechanism that allows the decoder to attend to the encoded input representation when generating the output sequence.\n",
        "\n",
        "The input embedding is processed by each layer of the encoder block, the final out of which is then processed by each layer of the decoder block, as illustrated below.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1-oieNTlJhp7owIbEzwfLppeqyunYsHA2)\n",
        "\n",
        "\n",
        "A Transformer can consist of either or both blocks, depending on the required task:\n",
        "\n",
        "\n",
        "| Transformer Type | Use Case                                                                                                              |\n",
        "|------------------|-----------------------------------------------------------------------------------------------------------------------|\n",
        "| Encoder-only     | Good for tasks that require understanding of the input,  such as sentence classification and named entity recognition |\n",
        "| Decoder-only     | Good for generative tasks such as text generation                                                                     |\n",
        "| Encoder-Decoder  | Good for generative tasks that require an input,  such as translation or summarization                                |\n",
        "                          |\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNMp8B-DMHxo"
      },
      "source": [
        "# **1.2 - How can we use Transformers?**\n",
        "\n",
        "*(Expected reading time: 10 minutes)*\n",
        "\n",
        "###**1.21 - Training a Large Language Model (LLM)**\n",
        "Training a LLM  requires a vast amount of compute resources, energy, training data, technical expertise, and can last days or even weeks.\n",
        "\n",
        "For example, training of  [BigScience's BLOOM](https://huggingface.co/bigscience/bloom) model on a dataset of 46 natural languages and 13 programming languages required 384 80GB GPUs running for 3.5 months at a cost of up to $5M. \n",
        "\n",
        "Clearly, training a state-of-the-art model from scratch is beyond the scope of this course.\n",
        "\n",
        "\n",
        "###**1.22 - Pre-trained models**\n",
        "\n",
        "Fortunately, many pre-trained models are available openly online. These are sometimes run on cloud infrastructure and accessible through an API or the model weights can be downloaded from a repository so you can run them locally. [Huggingface](https://huggingface.co) provides such a model repository service.\n",
        "\n",
        "Larger models have a file size in the range of GBs and billions of parameters. BLOOM, for example, is one of the largest models available in the HuggingFace library, and has a total size of around 330GB and 175 billion parameters.\n",
        "\n",
        "On the other hand, smaller models, such as BERT-base, can have a file size in the range of hundreads of MBs and millions of parameters. For example, BERT-base-uncased is around 400MB and 110 million parameters.\n",
        "\n",
        "\n",
        "###**1.23 - NLP Tasks**\n",
        "\n",
        "Transformers excel at a wide variety of NLP tasks. Transformer models can be general-purpose or intended for a specific task. As discussed in the next section, many models are freely available online, with 100s or 1000s of different ones available for each task.\n",
        "\n",
        "\n",
        "A general-purpose model can also be \"fine-tuned\" (re-trained with additional data) to accomplish a desired task, as discussed in the next section.\n",
        "\n",
        "The below image summarises the NLP tasks that Transformers have been applied to, and the number of models available for them [here](https://huggingface.co/tasks).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=18LLmal_CEIn6POlFL-tSBjFyxCg_0Vz0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KECBO7K9a6IM"
      },
      "source": [
        "# **1.3 - Hugging Face** ðŸ¤—\n",
        "\n",
        "Hugging Face is a company that provides an easy-to-use platform for training, evaluating, and using state-of-the-art NLP models. They have a wide variety of pre-trained models available to download and use, which can save you a lot of time and resources. These models have been trained on massive amounts of data and can understand and generate human-like text.\n",
        "\n",
        "One of the most important things they provide is Hugging Face Transformers. It's a library that allows you to easily use, fine-tune and train transformer-based language models like BERT, GPT-2, RoBERTa, etc.\n",
        "It's like having a team of NLP experts at your fingertips. With Hugging Face Transformers, you can easily load pre-trained models, tokenize text, and perform various NLP tasks like text classification, language translation, and text generation with just a few lines of code. It's user-friendly, well-documented and you can use it to experiment with different models and tasks in no time!\n",
        "\n",
        "### **1.31 - Models**\n",
        "These are the pre-trained NLP models that you can use for various tasks like text classification, language translation, and text generation. These models have been trained on massive amounts of data and can understand and generate human-like text. You can use these models as is, or fine-tune them for your specific use case.\n",
        "\n",
        "### **1.32 - Tokenizers**\n",
        "These are tools that allow you to break down text into smaller units, such as words or subwords, which is an important step when working with NLP models. Hugging Face provides a wide variety of tokenizers that you can use, including BERT tokenizer, GPT tokenizer, and many more.\n",
        "\n",
        "### **1.33 - Pipelines**\n",
        "Think of pipelines as pre-built, ready-to-use NLP workflows that you can use for various tasks such as text classification, question answering, and text generation. These pipelines are built on top of pre-trained models, tokenizers, and other components, and they allow you to perform complex NLP tasks with just a few lines of code.\n",
        "\n",
        "For example, imagine you want to classify a piece of text as positive or negative, you can use the `sentiment-analysis` pipeline, just by calling the pipeline with the text, it will tokenize the text, then it will use the pre-trained model to classify the text, and finally, it will return the sentiment. It's like having a magic wand that can do wonders with your text data.\n",
        "\n",
        "\n",
        "***Try combining these aspects of the ðŸ¤— API by running the code below:***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Download pre-trained model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Create text classification pipeline\n",
        "classification_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Use pipeline to classify text\n",
        "result = classification_pipeline(\"This is an example of a positive text.\")\n",
        "\n",
        "# Print result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uzGPaCTJzeNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.4 - Fine-tuning a pre-trained model**\n",
        "Fine-tuning involves adapting a pre-trained model to a specific task by training it on a smaller dataset. The process of fine-tuning a pre-trained model is typically divided into three main steps:\n",
        "\n",
        "**1. Loading the pre-trained model**: The first step is to load the pre-trained model from a checkpoint or from a library like HuggingFace's Transformers. This can be done using the `from_pretrained()` function from the transformers library. For example, to load the pre-trained BERT model from HuggingFace, you would use the following code:\n",
        "\n",
        "```python\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "```\n",
        "**2. Preparing the dataset**: The next step is to prepare the dataset for fine-tuning. This typically involves loading the data, tokenizing it, and converting it into a format that can be fed into the model. For example, to fine-tune a BERT model for a text classification task, you would need to tokenize the text, convert it into input ids and attention masks using the tokenizer, and also convert the labels into one-hot encoded format.\n",
        "```python\n",
        "# Import required libraries\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare the dataset\n",
        "# Here, we'll assume that we have a list of texts and labels called `texts` and `labels` respectively\n",
        "# We'll use sklearn's train_test_split function to split the data into train and test sets\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "# Tokenize the texts and convert them into input ids and attention masks\n",
        "input_ids_train = [tokenizer.encode(text, add_special_tokens=True) for text in texts_train]\n",
        "attention_masks_train = [[1] * len(input_id) for input_id in input_ids_train]\n",
        "\n",
        "# Convert labels into one-hot encoded format\n",
        "labels_train = torch.tensor([[label] for label in labels_train])\n",
        "\n",
        "# Create DataLoader\n",
        "train_data = TensorDataset(torch.tensor(input_ids_train), torch.tensor(attention_masks_train), labels_train)\n",
        "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "```\n",
        "\n",
        "**3. Fine-tuning the model**: The final step is to fine-tune the model by training it on the prepared dataset. This is done by setting the model in training mode, defining a loss function and an optimizer, and training the model for a certain number of epochs. For example, to fine-tune a BERT model for text classification, you would use the following code:\n",
        "\n",
        "```python\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AdamW\n",
        "\n",
        "# Set model in training mode\n",
        "model.train()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Unpack the batch\n",
        "        b_input_ids, b_attention_masks, b_labels = batch\n",
        "        \n",
        "        # Move the data to the GPU\n",
        "        b_input_ids = b_input_ids.to(device)\n",
        "        b_attention_masks = b_attention_masks.to(device)\n",
        "        b_labels = b_labels.to(device)\n",
        "        \n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = model(b_input_ids, attention_mask=b_attention_masks)[0]\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits.view(-1, num_labels), b_labels.view(-1))\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print progress\n",
        "        if step % 100 == 0:\n",
        "            print(f'Epoch {epoch + 1} | Step {step} | Loss: {loss.item()}')\n",
        "```\n",
        "---"
      ],
      "metadata": {
        "id": "2KxQEKHqzfDm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2GW8cGfJGci"
      },
      "source": [
        "# **Section 2 - Fine-Tuning Example Task**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we walk through a simple example on a small dataset. We will aim to create a model that works well on the 'poem_sentiment' dataset from the HuggingFace Dataset Hub (https://huggingface.co/datasets) - this may be a useful data source for your final assessment. \n"
      ],
      "metadata": {
        "id": "tcJXpTAqTOS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 - The Dataset**\n",
        "\n",
        "The first step of any Machine Learning prediction task is to analyse and understand the dataset you're working with. It is very important to only use the training dataset for this - no 'peeking' at the test dataset! It is usually useful to know:\n",
        "- Distribution of labels in our dataset\n",
        "- What does a random batch from the dataset 'look' like. This is helpful when building intuition about our model training. In non-NLP tasks this can include plotting histograms of distributions and analysing the data for trends that could potentially be included as features\n",
        "\n",
        "Inspecting the dataset is usually important as non-academic datasets will require extensive cleaning and pre-processing pipelines before they are able to be used in model training.   "
      ],
      "metadata": {
        "id": "zkvWQ-RTrPgi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbtcpjS5g5c3"
      },
      "source": [
        "# **2.2 - Training Transformer Models**\n",
        "\n",
        "**In the previous section you will have:**\n",
        "- Worked with transformer pipelines \n",
        "- Loaded datasets, tokenizers and models\n",
        "\n",
        "**In this section you will:**\n",
        "- Understand the difference between fine tuning and pre-training.\n",
        "- Fine-tune a pre-trained transformer model from scratch as per the example code from the hugging face tutorial on a text classification setting.\n",
        "- Compare the fine-tuned model results to those directly from a pipeline.\n",
        "\n",
        "**If you have time:**\n",
        "- Learn how to create a custom dataset loader, such that you can easily adapt any dataset for their project work"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#We first load the dataset:\n",
        "raw_dataset = load_dataset(\"poem_sentiment\")\n",
        "print(raw_dataset)\n",
        "\n",
        "# We can examine individual data points by indexing the dataset object\n",
        "print(raw_dataset['train'][0])\n",
        "print(raw_dataset['train'].features)\n",
        "\n",
        "#Ensure the dataset is formatted for pytorch:\n",
        "raw_dataset.set_format('torch')"
      ],
      "metadata": {
        "id": "1Wii7rMwsoDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.21 -** *TO DO:*\n",
        "Do some quick rudimentary dataset analysis, this should include looking at the label distribution in the training dataset. Use whatever python libraries you're comfortable with. What do the various labels mean?"
      ],
      "metadata": {
        "id": "zlk1kahUvKP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now load the pipeline API from the HuggingFace `transformers` API. We evaluate the model's performance on the validation dataset."
      ],
      "metadata": {
        "id": "2qlAJ6xEtCqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "raw_val_dataset = raw_dataset['validation']['verse_text']\n",
        "pipeline_output = classifier(raw_val_dataset)\n",
        "\n",
        "#Parse the pipeline into a similar format to the dataset:\n",
        "def parse_labels(l):\n",
        "  if l == 'NEGATIVE':\n",
        "    return 0\n",
        "  elif l == 'POSITIVE':\n",
        "    return 1\n",
        "  else:\n",
        "    raise NotImplementedError()\n",
        "\n",
        "#Note the pipeline already constrains us from defining labels such as 'mixed' and 'other found in our dataset:\n",
        "pipeline_output_labels = np.array(list(map(lambda x: parse_labels(x['label']), pipeline_output)))\n",
        "pipeline_output_labels"
      ],
      "metadata": {
        "id": "C2HdYvrXtL7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.22 -** *TO DO:* \n",
        "Look at the pipeline model outputs and dataset labels what are some issues with using the out-of-the-box pipeline models?\n",
        "\n",
        "How might we evaluate the performance of the pipeline model on the dataset given these issues?"
      ],
      "metadata": {
        "id": "NOC8xvY7u3O1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Evaluate the performance of the pipeline model on the validation dataset "
      ],
      "metadata": {
        "id": "PNK9W9S-uzhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 - Fine-Tuning our own Model**\n",
        "\n",
        "In order to specifically train a model to the requirements of our dataset we will now look at how we might fine-tune a LLM on our training dataset. \n",
        "\n",
        "Please complete the sections marked *TODO* in the code below:  "
      ],
      "metadata": {
        "id": "FY6nW-nwv90c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "#First create the AutoTokenizer object:\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Apply the tokenizer to the dataset:\n",
        "tokenized_dataset = raw_dataset.map(lambda data_point: tokenizer(data_point['verse_text']))\n",
        "\n",
        "#TODO: Inspect a sample of the dataset after it has been Tokenized:\n",
        "\n",
        "#We need to remove the non-tokenized components of our dataset:\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['id', 'verse_text'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column('label', 'labels')"
      ],
      "metadata": {
        "id": "qwzB-WJUwNUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pprint \n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Finally we define a Data Collator:\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "#It is good practice to use the pytorch Dataloader API when possible\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_dataset['train'], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    tokenized_dataset['validation'], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "#Lets look at a sample from the dataloader:\n",
        "sample = next(iter(train_dataloader))\n",
        "\n",
        "#Is the dimensionality of our data correct (hint ... yes but always worth checking)\n",
        "pprint.pprint({k: v.shape for k, v in sample.items()}) "
      ],
      "metadata": {
        "id": "8YkszUlcxAvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "#Using the HuggingFace Documentation find the correct arguments for loading the model:\n",
        "model = AutoModelForSequenceClassification.from_pretrained(###TODO###)"
      ],
      "metadata": {
        "id": "mTvsP7fExalZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##TODO: Using our sample variable check the loaded model works. Without creating a gradient tree check the outputs of the pre-trained model "
      ],
      "metadata": {
        "id": "BintRKRUxpHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import Adam\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "#Number of hyperparameters \n",
        "num_epochs=5\n",
        "progress_bar = tqdm(range(num_epochs*len(train_dataloader)))\n",
        "optimizer = Adam(model.parameters(), lr=5e-5)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "#Setup recording:\n",
        "epoch_losses = []\n",
        "\n",
        "#Setup our training loop:\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  losses = []\n",
        "  for batch in train_dataloader:\n",
        "\n",
        "    #TODO: Write a standard pytorch training loop to update the model parameters via a backward pass\n",
        "\n",
        "    progress_bar.update(1)\n",
        "\n",
        "    #record our loss per epoch\n",
        "    losses.append(loss.cpu().detach())\n",
        "    #break; #remove break to fully train model\n",
        "\n",
        "  mean_losses = np.mean(losses)\n",
        "  print(f'epoch {epoch} complete, loss: {mean_losses}')\n",
        "  epoch_losses.append(mean_losses)\n",
        "\n",
        "  fig, axs = plt.subplots()\n",
        "  axs.plot(epoch_losses)"
      ],
      "metadata": {
        "id": "zYbxxBYlx9z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate our model performance on the validation set:\n",
        "validation_loss = []\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch in val_dataloader:\n",
        "\n",
        "    outputs = model(**batch)\n",
        "    logits = outputs.logits \n",
        "\n",
        "    predictions.append(torch.argmax(logits, dim=-1))\n",
        "    labels.append(batch['labels'])\n",
        "    validation_loss.append(outputs.loss)\n",
        "\n",
        "val_loss = torch.stack(validation_loss).mean()\n",
        "print(val_loss)"
      ],
      "metadata": {
        "id": "38pPfS3OyfRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##TODO: Compare suitable evaluation metrics of the two models we've worked with. Discuss the advantages and disadvantages of both methods"
      ],
      "metadata": {
        "id": "gIfQnTtOywLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re2o-fsXdaXB"
      },
      "source": [
        "---\n",
        "# **Section 3 - Select Dataset & Model, Fine-Tune, Evaluate**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyO_VtXndeZv"
      },
      "source": [
        "For this task we will give you three separate datasets to investigate. Firstly look at each of these datasets and understand what they consist of and what NLP tasks you could fine-tune using these datasets.\n",
        "\n",
        "Then, we ask you to do the following:\n",
        "\n",
        "\n",
        "**1. Look at the three datasets below on Hugging Face and investigate them thouroughly. Understand these following aspects before you move on:**\n",
        "* What features do the datasets contain?\n",
        "* Are the datasets already tokenised or do they contain text?\n",
        "* What are suitable tasks to train these datasets on? (e.g. Token classification, sentiment analysis, sequence classification, masked language modelling)\n",
        "\n",
        "**2. Choose one of these datasets to finetune a model.**\n",
        "> Understand what the task it is you are going to fine tune on given the dataset.\n",
        "\n",
        "**3. Choose a model to finetune on this dataset.**\n",
        "\n",
        "> Use the Hugging Face documentation to choose a correct model ([Hugging Face models](https://huggingface.co/models))\n",
        "\n",
        "**4.   Pre-process the dataset to train the model.**\n",
        "> Understanding exactly what task you are going to be fine-tuning the model for will help a lot here. \n",
        "Think about what is it the model needs as an input and see how you need to change the given features to these inputs.\n",
        " Use Hugging Face tokenisers, data collators and general documentation to figure this out.\n",
        "\n",
        "**5. Train the model on this dataset.**\n",
        "> Use a manual training loop here, understand the mechanics behind training and implement it yourself (you can find this in the Hugging Face documentation and in the example from section 1 of this notebook).\n",
        "\n",
        "**6. Evaluate the new model's performance - compare with the performance of the model before fine-tuning.**\n",
        "> Look at what metric you would use to measure the performance of the model, this can be tricky for some language modelling tasks with non-deterministic labels.\n",
        "\n",
        "\n",
        "The datasets:\n",
        "\n",
        "1. [tweet_eval](https://huggingface.co/datasets/tweet_eval)\n",
        "2. [wikitext](https://huggingface.co/datasets/wikitext)\n",
        "3. [wikiann](https://huggingface.co/datasets/wikiann)\n",
        "\n",
        "Models available at: https://huggingface.co/models"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}